{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFFePzBhuh4O",
        "outputId": "30c8cf09-4879-4110-81c2-3ec11a6d5daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install --quiet underthesea\n",
        "! pip install --quiet vncorenlp\n",
        "! pip install --quiet py_vncorenlp\n",
        "! pip install --quiet python-rdrsegmenter\n",
        "! pip install scikit-multilearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "KRr76qRsZY7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhS3Y1BK0LHE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "1ec7633c-a5df-4ccd-f501-83d6ee359ab9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'train = pd.read_excel(\"/content/drive/MyDrive/Thesis: Topic Modelling/Data/Splitted data/train.xlsx\")\\ntest = pd.read_excel(\"/content/drive/MyDrive/Thesis: Topic Modelling/Data/Splitted data/test.xlsx\")\\n\\ntrain, valid = train_test_split(train, test_size = 0.1, random_state = 60)\\ntrain.to_excel(\"/content/drive/MyDrive/Thesis: Topic Modelling/Data/Splitted data/train.xlsx\")\\nvalid.to_excel(\"/content/drive/MyDrive/Thesis: Topic Modelling/Data/Splitted data/valid.xlsx\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\"\"\"train = pd.read_excel(\"/content/drive/MyDrive/Thesis: Topic Modelling/Data/Splitted data/train.xlsx\")\n",
        "test = pd.read_excel(\"/content/drive/MyDrive/Thesis: Topic Modelling/Data/Splitted data/test.xlsx\")\n",
        "\n",
        "train, valid = train_test_split(train, test_size = 0.1, random_state = 60)\n",
        "train.to_excel(\"/content/drive/MyDrive/Thesis: Topic Modelling/Data/Splitted data/train.xlsx\")\n",
        "valid.to_excel(\"/content/drive/MyDrive/Thesis: Topic Modelling/Data/Splitted data/valid.xlsx\")\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Njb23nhzvFof"
      },
      "outputs": [],
      "source": [
        "# see data preprocessing steps at link:\n",
        "# https://colab.research.google.com/drive/1AuvWD-Lo6UIjyb_fmjxGB1zLZWZQ7GFf?ouid=115125045212634117084&usp=drive_link\n",
        "from distutils.dir_util import copy_tree\n",
        "copy_tree(\"/content/drive/MyDrive/Thesis: Topic Modelling/Code/utils\", \"./utils/\")\n",
        "\n",
        "from utils.data_preprocessing_v2 import *\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from underthesea import word_tokenize, text_normalize\n",
        "from gensim.models import word2vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, confusion_matrix\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from scipy.stats import mode\n",
        "from vncorenlp import VnCoreNLP\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Thesis: Topic Modelling/Code/utils/vietnamese-stopwords.txt\") as f:\n",
        "    STOPWORDS = f.readlines()\n",
        "    STOPWORDS = [remove_all_tag(i) for i in STOPWORDS]\n",
        "\n",
        "\n",
        "def evaluate(y_true, y_pred, eval_individual = True, model_name = \"\"):\n",
        "  label_cols = [\"Quality\", \"Pack\", \"Serve\", \"Shipping\", \"Price\", \"Other\"]\n",
        "  print(f\"Classification report from {model_name}\")\n",
        "  print(classification_report(y_true[label_cols], y_pred))\n",
        "  print('Hamming Loss: ', round(hamming_loss(y_true, y_pred),3))\n",
        "  if eval_individual:\n",
        "    for i in range(len(label_cols)):\n",
        "      print(f\"classification report of {label_cols[i]}\")\n",
        "      print(classification_report(y_true[label_cols[i]], y_pred[:, i]))\n",
        "\n",
        "\n",
        "def evaluate_chain(model, X_train, y_train, X_test, y_test, predict_proba = True):\n",
        "  chains = [ClassifierChain(model, order=\"random\", random_state=i) for i in range(10)]\n",
        "  ovr = OneVsRestClassifier(model)\n",
        "  ovr.fit(X_train, y_train)\n",
        "  Y_pred_ovr = ovr.predict(X_test)\n",
        "  ovr_hamming_loss = hamming_loss(y_test, Y_pred_ovr)\n",
        "  for chain in chains:\n",
        "    chain.fit(X_train, y_train)\n",
        "  if predict_proba:\n",
        "    Y_pred_chains = np.array([chain.predict_proba(X_test) for chain in chains])\n",
        "    Y_pred_ensemble = Y_pred_chains.mean(axis=0)\n",
        "    Y_pred_ensemble = Y_pred_ensemble >= 0.5\n",
        "  else:\n",
        "    Y_pred_chains = np.array([chain.predict(X_test) for chain in chains])\n",
        "    Y_pred_ensemble = mode(Y_pred_chains, axis = 0)[0]\n",
        "  chain_hamming_loss = [\n",
        "      hamming_loss(y_test, Y_pred_chain >= 0.5)\n",
        "      for Y_pred_chain in Y_pred_chains\n",
        "  ]\n",
        "  ensemble_hamming_loss = hamming_loss(y_test, Y_pred_ensemble)\n",
        "  scores = [ovr_hamming_loss] + chain_hamming_loss + [ensemble_hamming_loss]\n",
        "  evaluate(y_test, Y_pred_ovr, eval_individual = False, model_name = \"One Vs Rest model\")\n",
        "  evaluate(y_test, Y_pred_ensemble, eval_individual = False, model_name = \"Ensemble model from classifier chain\")\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_excel(\"/content/drive/MyDrive/Thesis: Topic Modelling/Data/Splitted data/train.xlsx\")\n",
        "test = pd.read_excel(\"/content/drive/MyDrive/Thesis: Topic Modelling/Data/Splitted data/test.xlsx\")\n",
        "valid = pd.read_excel(\"/content/drive/MyDrive/Thesis: Topic Modelling/Data/Splitted data/valid.xlsx\")\n",
        "\n",
        "for i in [train,test, valid]:\n",
        "  print(i.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtMgY3LpEgkW",
        "outputId": "7cdeab47-2a5b-412c-a5cf-8a0b9d02a98f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12240, 11)\n",
            "(3401, 10)\n",
            "(1361, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc8SCKmI1t2P"
      },
      "outputs": [],
      "source": [
        "labels_cols = [\"Quality\", \"Pack\", \"Serve\", \"Shipping\", \"Price\", \"Other\"]\n",
        "train = pd.read_excel(\"/content/drive/MyDrive/Thesis: Topic Modelling/Data/Splitted data/train.xlsx\")\n",
        "train[\"comment\"] = train[\"comment\"].astype(str)\n",
        "train[\"comment\"] = train[\"comment\"].apply(lambda x: cleaning(x))\n",
        "X_train = train[\"comment\"]\n",
        "y_train = train[labels_cols]\n",
        "\n",
        "test = pd.read_excel(\"/content/drive/MyDrive/Thesis: Topic Modelling/Data/Splitted data/test.xlsx\")\n",
        "test[\"comment\"] = test[\"comment\"].astype(str)\n",
        "test[\"comment\"] = test[\"comment\"].apply(lambda x: cleaning(x))\n",
        "X_test = test[\"comment\"]\n",
        "y_test = test[labels_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz0IRfwt2NB1",
        "outputId": "a605879a-2261-4bf0-91f6-ab1158c3e885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'ai', 'alô', 'amen', 'anh', 'ba', 'ba ba', 'bao', 'bao giờ', 'bao lâu', 'bao nhiêu', 'bay', 'biến', 'biết', 'biết bao', 'biết bao nhiêu', 'biết đâu', 'buổi', 'bà', 'bài', 'bác', 'bán', 'bán dạ', 'bây', 'bây bẩy', 'bây giờ', 'bây nhiêu', 'bèn', 'béng', 'bên', 'bông', 'bước', 'bạn', 'bản', 'bản thân', 'bất', 'bất chợt', 'bất cứ', 'bất giác', 'bất kì', 'bất kể', 'bất kỳ', 'bất luận', 'bất ngờ', 'bất nhược', 'bất quá', 'bất thình', 'bất tử', 'bất ý', 'bất đồ', 'bấy', 'bấy chầy', 'bấy chừ', 'bấy giờ', 'bấy lâu', 'bấy lâu nay', 'bấy nay', 'bấy nhiêu', 'bập', 'bập bõm', 'bắt đầu', 'bằng', 'bển', 'bệt', 'bị', 'bỏ', 'bỗng', 'bỗng dưng', 'bỗng nhiên', 'bộ', 'bội', 'bớ', 'bởi', 'bởi thế', 'bởi vì', 'bởi vậy', 'bức', 'cao', 'cao ráo', 'cao thế', 'cha', 'chao', 'chi', 'chia sẻ', 'chiếc', 'cho', 'cho hay', 'cho nên', 'choa', 'chu cha', 'chui', 'chung', 'chuyển', 'chuyện', 'chuẩn bị', 'chà', 'chành', 'chí', 'chính', 'chót', 'chùn', 'chú', 'chúng', 'chúng ta', 'chúng tôi', 'chăn', 'chăng', 'chũn', 'chơi', 'chưa', 'chưng', 'chạnh', 'chả', 'chầm', 'chậc', 'chập', 'chắc', 'chắc chắn', 'chắc dạ', 'chắc hẳn', 'chắn', 'chẳng', 'chẳng lẽ', 'chẳng những', 'chết', 'chỉ', 'chỉn', 'chị', 'chịu', 'chọn', 'chốc', 'chốc chốc', 'chớ', 'chợt', 'chủn', 'chứ', 'chừ', 'chừng', 'chừng nào', 'coi', 'con', 'cu', 'cuối', 'cuối cùng', 'cuốn', 'cuộc', 'càng', 'cá nhân', 'các', 'cách', 'cái', 'câu', 'cây', 'còn', 'có', 'có khi', 'có thể', 'có vẻ', 'cóc', 'cô', 'công nhiên', 'cùng', 'cùng cực', 'căn', 'cũng', 'cơ', 'cơ hội', 'cơn', 'cạnh', 'cả', 'cảm thấy', 'cảm ơn', 'cấp', 'cần', 'cật', 'cật lực', 'cậu', 'cắt', 'cổ', 'cụ thể', 'cục', 'của', 'cứ', 'cứ điểm', 'cực', 'cực lực', 'da', 'do', 'do vậy', 'do đó', 'duy', 'dà', 'dài', 'dành', 'dào', 'dì', 'dù', 'dùng', 'dĩ', 'dưới', 'dạ', 'dạ dạ', 'dần', 'dần dần', 'dầu', 'dẫn', 'dẫu', 'dẫu sao', 'dễ', 'dở', 'dữ', 'em', 'giá trị', 'giảm', 'giống', 'giờ', 'giờ đây', 'giời', 'giữ', 'giữa', 'gây', 'gì', 'gần', 'gặp', 'gồm', 'ha', 'hay', 'hiểu', 'hiện nay', 'hiện tại', 'hoàn toàn', 'hoặc', 'hãy', 'hèn', 'hô', 'hơn', 'hơn nữa', 'hầu', 'hầu hết', 'hậu', 'hẳn', 'hết', 'họ', 'hỏi', 'hồ', 'hỗ trợ', 'hự', 'khi', 'khiến', 'khoảng', 'khoảng cách', 'khá', 'khác', 'khách', 'khó', 'khó khăn', 'khói', 'khô', 'không', 'không chỉ', 'không những', 'không thể', 'khẳng định', 'khỏi', 'kê', 'kìa', 'kể', 'kể cả', 'lai', 'le', 'liên quan', 'loại', 'loạt', 'luôn', 'luôn luôn', 'luật', 'luốt', 'là', 'làm', 'làm sao', 'lâu', 'lâu lâu', 'lâu nay', 'lên', 'lên cơn', 'lình', 'lí', 'lòng', 'lô', 'lúc', 'lúc nào', 'lý', 'lý do', 'lượng', 'lại', 'lấy', 'lần', 'lẽ', 'lị', 'lớn', 'lớn nhỏ', 'lời', 'lời nói', 'lự', 'lực', 'mang', 'muốn', 'mà', 'mày', 'mình', 'mòi', 'mù', 'mạng', 'mạnh', 'mất', 'mấy', 'mẹ', 'mọi', 'mối', 'mỗi', 'một', 'một cách', 'một khi', 'một số', 'một vài', 'một ít', 'mới', 'mới đây', 'mở', 'mợ', 'mức', 'mực', 'nay', 'ngay', 'ngay lập tức', 'nghe', 'nghe đâu', 'nghen', 'nghiễm nhiên', 'nghĩ', 'nghỉm', 'ngoài', 'ngoài ra', 'ngoải', 'nguồn', 'ngày', 'ngày càng', 'ngày ngày', 'ngày xưa', 'ngôi', 'ngõ', 'ngăn', 'ngươi', 'người', 'người người', 'ngắt', 'ngọn', 'ngọt', 'ngồi', 'ngồi bệt', 'ngộ', 'nhanh', 'nhau', 'nhiên', 'nhiều', 'nhiệt liệt', 'nhung', 'nhà', 'nhà ngươi', 'nhân dịp', 'nhân tiện', 'nhé', 'nhén', 'nhìn', 'nhìn chung', 'nhìn nhận', 'nhóm', 'nhón', 'nhăng', 'như', 'như chơi', 'như thế nào', 'như thể', 'nhưng', 'nhưng mà', 'nhược', 'nhất', 'nhất quyết', 'nhất thiết', 'nhất định', 'nhận', 'nhằm', 'nhỉ', 'nhỏ', 'nhớ', 'nhờ', 'nhỡ', 'những', 'nào', 'này', 'nên', 'nó', 'nóc', 'nói', 'nói chung', 'nói riêng', 'năm', 'nơi', 'nước', 'nả', 'nấy', 'nặng', 'nếu', 'nếu như', 'nền', 'nọ', 'nỗi', 'nớ', 'nở', 'nức', 'nữa', 'oai', 'oái', 'phi', 'pho', 'phui', 'phè', 'phía', 'phóc', 'phót', 'phù hợp', 'phăn', 'phương', 'phải', 'phần', 'phần lớn', 'phần nhiều', 'phần nào', 'phắt', 'phỉ', 'phỏng', 'phốc', 'phụt', 'phứt', 'qua', 'qua lại', 'quan trọng', 'quan tâm', 'quay', 'quy', 'quy lại', 'quá', 'quá trình', 'quả', 'quả thật', 'quận', 'ra', 'ra tay', 'ren', 'riu', 'riêng', 'riệt', 'rày', 'ráo', 'rén', 'rích', 'ríu', 'rón', 'rõ', 'rút', 'răng', 'rất', 'rằng', 'rốt', 'rốt cuộc', 'rồi', 'rứa', 'sa', 'sang', 'sao', 'sau', 'sau cùng', 'sau này', 'sinh', 'so', 'song', 'suýt', 'sáng', 'sì', 'sả', 'sất', 'sắp', 'sẽ', 'số', 'sốt', 'sột', 'sớm', 'sở dĩ', 'sợ', 'sức', 'sử dụng', 'sự', 'sự việc', 'tanh', 'tay', 'te', 'tha hồ', 'than', 'thanh', 'thanh thanh', 'thay đổi', 'theo', 'thi thoảng', 'thiên địa', 'thiếu', 'thoạt', 'thoạt nhiên', 'thoắt', 'thuần', 'thuần ái', 'thuộc', 'thà', 'thành', 'thái', 'tháng', 'tháo', 'thêm', 'thì', 'thình', 'thích', 'thím', 'thôi', 'thôi việc', 'thúng', 'thương', 'thường', 'thảo', 'thảy', 'thấp', 'thấp thỏm', 'thấy', 'thẩy', 'thậm', 'thậm chí', 'thật', 'thật ra', 'thật sự', 'thật thà', 'thắng', 'thế', 'thế nào', 'thếch', 'thể', 'thỉnh thoảng', 'thị', 'thỏm', 'thốc', 'thốt', 'thốt nhiên', 'thộc', 'thời gian', 'thời điểm', 'thục', 'thứ', 'thử', 'thửa', 'thực', 'thực hiện', 'thực ra', 'thực sự', 'thực tế', 'tin', 'tiếp', 'tiếp theo', 'tiếp tục', 'tiện thể', 'tiệt', 'toẹt', 'trong', 'trung', 'tránh', 'tráo', 'trên', 'trên dưới', 'trước', 'trước hết', 'trước kia', 'trước tiên', 'trạo', 'trả', 'trếu', 'trển', 'trệt', 'trệu', 'trọi', 'trỏng', 'trời đất', 'trở thành', 'trừ', 'trực tiếp', 'tuy', 'tuy nhiên', 'tuy vậy', 'tuyệt nhiên', 'tuần', 'tuốt', 'tuồn', 'tuồng', 'tuổi', 'tuột', 'tà', 'tàn', 'tàn tán', 'tán', 'tâm', 'tê', 'tên', 'tênh', 'tì', 'tìm', 'tìm hiểu', 'tình trạng', 'tính', 'tính cách', 'tít', 'tò', 'tòa', 'tóe', 'tôi', 'tông tốc', 'tù', 'tăm tắp', 'tăng', 'tại', 'tại sao', 'tạo', 'tả', 'tấm', 'tấn', 'tất', 'tất cả', 'tất thảy', 'tất tần tật', 'tất tật', 'tập trung', 'tắp', 'tắp lự', 'tắp tắp', 'tề', 'tọt', 'tỏ', 'tỏ vẻ', 'tốc', 'tối', 'tốt', 'tột', 'tớ', 'tới', 'tức', 'tức khắc', 'tức thì', 'tức tốc', 'từ', 'từ từ', 'từng', 'tự', 'tự ý', 'tựu', 'veo', 'veo veo', 'việc', 'vung', 'và', 'vài', 'vài ba', 'vào', 'vào khoảng', 'vâng', 'vâng dạ', 'vèo', 'vì', 'vì thế', 'vì vậy', 'ví', 'vô', 'vô hình trung', 'vô luận', 'vô vàn', 'vùng', 'văng', 'vượt', 'vạn', 'vả', 'vả lại', 'vấn đề', 'vẫn', 'vậy', 'về', 'vị', 'vị trí', 'vốn', 'với', 'vở', 'vụt', 'vừa', 'vừa mới', 'vừa qua', 'vừa rồi', 'xa', 'xa tắp', 'xa xa', 'xem', 'xem lại', 'xem ra', 'xin', 'xiết', 'xon', 'xoành', 'xoét', 'xoạch', 'xoẳn', 'xoẹt', 'xuất hiện', 'xuất kì', 'xuất kỳ', 'xuể', 'xuống', 'xón', 'xúi', 'xăm', 'xả', 'xảy', 'xắm', 'xềnh', 'xệch', 'xệp', 'xử lý', 'xửa', 'yêu cầu', 'à', 'ào', 'ào ào', 'á', 'ái', 'ái chà', 'áng', 'âu', 'ít', 'ít nhiều', 'ít nhất', 'ô', 'ôi', 'ông', 'úi', 'úi dào', 'ý', 'ăn', 'đang', 'đi', 'điều', 'điều kiện', 'điểm', 'đành', 'đán', 'đáng', 'đáng kể', 'đánh', 'đánh giá', 'đáo', 'đâu', 'đâu đâu', 'đâu đó', 'đây', 'đã', 'đó', 'đùng', 'đúng', 'đơn vị', 'đưa', 'được', 'đạch', 'đại', 'đại nhân', 'đại phàm', 'đạt', 'đảm bảo', 'đấy', 'đầu tiên', 'đầy', 'đặc biệt', 'đặt', 'đến', 'đến nỗi', 'đều', 'để', 'đối với', 'đồng thời', 'đủ', 'ơ', 'ơi', 'ư', 'ạ', 'ấy', 'ầu', 'ắt', 'ối', 'ối dào', 'ồ', 'ổng', 'ớ', 'ờ', 'ở', 'ủa', 'ứ', 'ừ'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "bow = CountVectorizer(ngram_range=(1, 5), tokenizer=word_tokenize, stop_words=STOPWORDS,max_df=0.5, min_df=4)\n",
        "X_train_bow = bow.fit_transform(X_train)\n",
        "X_test_bow = bow.transform(X_test)\n",
        "\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 5), tokenizer=word_tokenize, stop_words=STOPWORDS,max_df=0.5, min_df=4, smooth_idf=True)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgCXWMoh4Ton"
      },
      "outputs": [],
      "source": [
        "rdrsegmenter = VnCoreNLP(\"/content/drive/MyDrive/transformers/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
        "\n",
        "X_train_tokenized = [rdrsegmenter.tokenize(corpus)[0] for corpus in X_train.values]\n",
        "X_test_tokenized = [rdrsegmenter.tokenize(corpus)[0] for corpus in X_test.values]\n",
        "vs = 150 # number of dimensions that our words are going to be embedded in\n",
        "context_size = 5 # context window\n",
        "min_word = 10 # minimal number of occurence to be included in the embedded corpus\n",
        "\n",
        "skipgram = word2vec.Word2Vec(X_train_tokenized, vector_size = vs, \\\n",
        "                            window=context_size, min_count=min_word, \\\n",
        "                            epochs=50, seed=42, sg = 1)\n",
        "\n",
        "cbow = word2vec.Word2Vec(X_train_tokenized, vector_size = vs, \\\n",
        "                            window=context_size, min_count=min_word, \\\n",
        "                            epochs=50, seed=42, sg = 0)\n",
        "\n",
        "# Get words and indexes from the word2vec model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4k4btqH_XKR"
      },
      "outputs": [],
      "source": [
        "word_vec_unpack = [(word, idx) for word, idx in \\\n",
        "                   skipgram.wv.key_to_index.items()]\n",
        "tokens, indexes = zip(*word_vec_unpack)\n",
        "word_vec_df = pd.DataFrame(skipgram.wv.vectors[indexes, :], index=tokens)\n",
        "X_train_skipgram = np.array([word_vec_df.loc[list(set(doc).intersection(set(word_vec_df.index)))].mean(axis=0) for doc in X_train_tokenized])\n",
        "X_test_skipgram = np.array([word_vec_df.loc[list(set(doc).intersection(set(word_vec_df.index)))].mean(axis=0) for doc in X_test_tokenized])\n",
        "\n",
        "\n",
        "word_vec_unpack = [(word, idx) for word, idx in \\\n",
        "                   cbow.wv.key_to_index.items()]\n",
        "tokens, indexes = zip(*word_vec_unpack)\n",
        "word_vec_df = pd.DataFrame(cbow.wv.vectors[indexes, :], index=tokens)\n",
        "X_train_cbow = np.array([word_vec_df.loc[list(set(doc).intersection(set(word_vec_df.index)))].mean(axis=0) for doc in X_train_tokenized])\n",
        "X_test_cbow = np.array([word_vec_df.loc[list(set(doc).intersection(set(word_vec_df.index)))].mean(axis=0) for doc in X_test_tokenized])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7H97IKw-irl"
      },
      "outputs": [],
      "source": [
        "X_train_skipgram = pd.DataFrame(X_train_skipgram).fillna(0)\n",
        "X_test_skipgram = pd.DataFrame(X_test_skipgram).fillna(0)\n",
        "X_train_cbow = pd.DataFrame(X_train_cbow).fillna(0)\n",
        "X_test_cbow = pd.DataFrame(X_test_cbow).fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aV3x0fn6RoP"
      },
      "outputs": [],
      "source": [
        "lr_model = LogisticRegression()\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "nb_model = MultinomialNB()\n",
        "sgd = SGDClassifier(loss='hinge', penalty='l2',\n",
        "  alpha=1e-3, random_state=42,\n",
        "  max_iter=5, tol=None)\n",
        "\n",
        "\n",
        "models_list = [(\"Logistic Regression\", lr_model), (\"XGBoost\", xgb_model), (\"Naive Bayes\", nb_model), (\"SVM\", sgd)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Evaluate Bow models\n"
      ],
      "metadata": {
        "id": "7RAdWyIHZcPn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9OSOQZU99ga",
        "outputId": "afe02b3d-42c2-441e-ffe3-def3fa14e8b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating Logistic Regression\n",
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.94      0.93      2721\n",
            "           1       0.96      0.92      0.94       581\n",
            "           2       0.92      0.80      0.85       518\n",
            "           3       0.93      0.89      0.91      1366\n",
            "           4       0.96      0.83      0.89       509\n",
            "           5       0.85      0.77      0.81       309\n",
            "\n",
            "   micro avg       0.93      0.89      0.91      6004\n",
            "   macro avg       0.92      0.86      0.89      6004\n",
            "weighted avg       0.93      0.89      0.91      6004\n",
            " samples avg       0.91      0.89      0.89      6004\n",
            "\n",
            "Hamming Loss:  0.052\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93      2721\n",
            "           1       0.96      0.91      0.94       581\n",
            "           2       0.90      0.81      0.85       518\n",
            "           3       0.93      0.89      0.91      1366\n",
            "           4       0.96      0.83      0.89       509\n",
            "           5       0.86      0.78      0.82       309\n",
            "\n",
            "   micro avg       0.92      0.90      0.91      6004\n",
            "   macro avg       0.92      0.86      0.89      6004\n",
            "weighted avg       0.92      0.90      0.91      6004\n",
            " samples avg       0.91      0.90      0.90      6004\n",
            "\n",
            "Hamming Loss:  0.051\n",
            "evaluating XGBoost\n",
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.95      0.92      2721\n",
            "           1       0.95      0.93      0.94       581\n",
            "           2       0.88      0.82      0.85       518\n",
            "           3       0.92      0.93      0.92      1366\n",
            "           4       0.96      0.88      0.92       509\n",
            "           5       0.84      0.72      0.78       309\n",
            "\n",
            "   micro avg       0.91      0.91      0.91      6004\n",
            "   macro avg       0.91      0.87      0.89      6004\n",
            "weighted avg       0.91      0.91      0.91      6004\n",
            " samples avg       0.89      0.91      0.89      6004\n",
            "\n",
            "Hamming Loss:  0.052\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.96      0.93      2721\n",
            "           1       0.94      0.94      0.94       581\n",
            "           2       0.90      0.83      0.86       518\n",
            "           3       0.92      0.93      0.93      1366\n",
            "           4       0.96      0.88      0.92       509\n",
            "           5       0.87      0.72      0.79       309\n",
            "\n",
            "   micro avg       0.91      0.92      0.92      6004\n",
            "   macro avg       0.92      0.88      0.89      6004\n",
            "weighted avg       0.91      0.92      0.92      6004\n",
            " samples avg       0.91      0.91      0.90      6004\n",
            "\n",
            "Hamming Loss:  0.049\n",
            "evaluating Naive Bayes\n",
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.90      0.90      2721\n",
            "           1       0.61      0.88      0.72       581\n",
            "           2       0.62      0.81      0.70       518\n",
            "           3       0.77      0.85      0.81      1366\n",
            "           4       0.80      0.79      0.80       509\n",
            "           5       0.74      0.55      0.63       309\n",
            "\n",
            "   micro avg       0.79      0.85      0.82      6004\n",
            "   macro avg       0.74      0.80      0.76      6004\n",
            "weighted avg       0.80      0.85      0.82      6004\n",
            " samples avg       0.79      0.85      0.80      6004\n",
            "\n",
            "Hamming Loss:  0.111\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.90      0.90      2721\n",
            "           1       0.61      0.87      0.72       581\n",
            "           2       0.64      0.81      0.72       518\n",
            "           3       0.77      0.85      0.81      1366\n",
            "           4       0.80      0.79      0.80       509\n",
            "           5       0.76      0.54      0.63       309\n",
            "\n",
            "   micro avg       0.79      0.85      0.82      6004\n",
            "   macro avg       0.75      0.79      0.76      6004\n",
            "weighted avg       0.80      0.85      0.82      6004\n",
            " samples avg       0.79      0.85      0.80      6004\n",
            "\n",
            "Hamming Loss:  0.11\n",
            "evaluating SVM\n",
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93      2721\n",
            "           1       0.96      0.92      0.94       581\n",
            "           2       0.91      0.77      0.83       518\n",
            "           3       0.92      0.92      0.92      1366\n",
            "           4       0.94      0.82      0.88       509\n",
            "           5       0.90      0.53      0.67       309\n",
            "\n",
            "   micro avg       0.93      0.88      0.90      6004\n",
            "   macro avg       0.93      0.81      0.86      6004\n",
            "weighted avg       0.93      0.88      0.90      6004\n",
            " samples avg       0.89      0.87      0.87      6004\n",
            "\n",
            "Hamming Loss:  0.055\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.94      0.92      2721\n",
            "           1       0.97      0.91      0.94       581\n",
            "           2       0.92      0.79      0.85       518\n",
            "           3       0.92      0.92      0.92      1366\n",
            "           4       0.96      0.83      0.89       509\n",
            "           5       0.88      0.53      0.66       309\n",
            "\n",
            "   micro avg       0.91      0.89      0.90      6004\n",
            "   macro avg       0.92      0.82      0.86      6004\n",
            "weighted avg       0.91      0.89      0.90      6004\n",
            " samples avg       0.90      0.88      0.88      6004\n",
            "\n",
            "Hamming Loss:  0.057\n"
          ]
        }
      ],
      "source": [
        "# bow evaluation\n",
        "\n",
        "\n",
        "score_dict = {}\n",
        "for name, model in models_list:\n",
        "  if name == \"SVM\":\n",
        "    predict_proba = 0\n",
        "  else:\n",
        "    predict_proba = 1\n",
        "  print(f\"evaluating {name}\")\n",
        "  score_dict[name] = evaluate_chain(model, X_train_bow, y_train, X_test_bow, y_test, predict_proba = predict_proba)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. TFIDF evaluation\n"
      ],
      "metadata": {
        "id": "2o-SGv4pZjn7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDbFLB_nFoKG",
        "outputId": "5c84dddf-c1f8-443c-b92f-16b2125f9dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating Logistic Regression\n",
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.98      0.92      2721\n",
            "           1       0.98      0.78      0.87       581\n",
            "           2       0.95      0.62      0.75       518\n",
            "           3       0.94      0.85      0.89      1366\n",
            "           4       0.97      0.61      0.75       509\n",
            "           5       0.93      0.42      0.58       309\n",
            "\n",
            "   micro avg       0.91      0.84      0.87      6004\n",
            "   macro avg       0.94      0.71      0.79      6004\n",
            "weighted avg       0.91      0.84      0.86      6004\n",
            " samples avg       0.88      0.84      0.85      6004\n",
            "\n",
            "Hamming Loss:  0.073\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.99      0.92      2721\n",
            "           1       0.98      0.77      0.87       581\n",
            "           2       0.95      0.62      0.75       518\n",
            "           3       0.94      0.86      0.90      1366\n",
            "           4       0.98      0.62      0.76       509\n",
            "           5       0.94      0.48      0.63       309\n",
            "\n",
            "   micro avg       0.90      0.85      0.87      6004\n",
            "   macro avg       0.94      0.72      0.80      6004\n",
            "weighted avg       0.91      0.85      0.87      6004\n",
            " samples avg       0.89      0.86      0.86      6004\n",
            "\n",
            "Hamming Loss:  0.072\n",
            "evaluating XGBoost\n",
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.95      0.93      2721\n",
            "           1       0.95      0.93      0.94       581\n",
            "           2       0.88      0.81      0.84       518\n",
            "           3       0.92      0.93      0.92      1366\n",
            "           4       0.97      0.87      0.92       509\n",
            "           5       0.86      0.72      0.78       309\n",
            "\n",
            "   micro avg       0.91      0.91      0.91      6004\n",
            "   macro avg       0.91      0.87      0.89      6004\n",
            "weighted avg       0.91      0.91      0.91      6004\n",
            " samples avg       0.89      0.91      0.89      6004\n",
            "\n",
            "Hamming Loss:  0.052\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.96      0.93      2721\n",
            "           1       0.94      0.93      0.94       581\n",
            "           2       0.90      0.83      0.86       518\n",
            "           3       0.92      0.92      0.92      1366\n",
            "           4       0.96      0.88      0.92       509\n",
            "           5       0.86      0.73      0.79       309\n",
            "\n",
            "   micro avg       0.91      0.92      0.92      6004\n",
            "   macro avg       0.91      0.88      0.89      6004\n",
            "weighted avg       0.91      0.92      0.91      6004\n",
            " samples avg       0.91      0.91      0.90      6004\n",
            "\n",
            "Hamming Loss:  0.05\n",
            "evaluating Naive Bayes\n",
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.99      0.91      2721\n",
            "           1       0.98      0.60      0.74       581\n",
            "           2       0.95      0.39      0.55       518\n",
            "           3       0.85      0.77      0.81      1366\n",
            "           4       0.99      0.35      0.52       509\n",
            "           5       0.97      0.19      0.32       309\n",
            "\n",
            "   micro avg       0.86      0.76      0.80      6004\n",
            "   macro avg       0.93      0.55      0.64      6004\n",
            "weighted avg       0.88      0.76      0.77      6004\n",
            " samples avg       0.84      0.77      0.78      6004\n",
            "\n",
            "Hamming Loss:  0.108\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      1.00      0.91      2721\n",
            "           1       0.98      0.59      0.74       581\n",
            "           2       0.95      0.39      0.55       518\n",
            "           3       0.84      0.77      0.80      1366\n",
            "           4       0.99      0.35      0.52       509\n",
            "           5       0.97      0.22      0.36       309\n",
            "\n",
            "   micro avg       0.86      0.76      0.80      6004\n",
            "   macro avg       0.93      0.55      0.65      6004\n",
            "weighted avg       0.88      0.76      0.77      6004\n",
            " samples avg       0.84      0.78      0.78      6004\n",
            "\n",
            "Hamming Loss:  0.109\n",
            "evaluating SVM\n",
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      1.00      0.90      2721\n",
            "           1       0.98      0.64      0.78       581\n",
            "           2       0.99      0.29      0.45       518\n",
            "           3       0.94      0.80      0.86      1366\n",
            "           4       0.98      0.42      0.59       509\n",
            "           5       0.93      0.13      0.22       309\n",
            "\n",
            "   micro avg       0.87      0.76      0.81      6004\n",
            "   macro avg       0.94      0.55      0.63      6004\n",
            "weighted avg       0.90      0.76      0.78      6004\n",
            " samples avg       0.85      0.77      0.79      6004\n",
            "\n",
            "Hamming Loss:  0.104\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      1.00      0.90      2721\n",
            "           1       0.99      0.65      0.78       581\n",
            "           2       0.98      0.31      0.47       518\n",
            "           3       0.94      0.80      0.87      1366\n",
            "           4       0.97      0.43      0.60       509\n",
            "           5       0.93      0.14      0.24       309\n",
            "\n",
            "   micro avg       0.87      0.77      0.81      6004\n",
            "   macro avg       0.94      0.55      0.64      6004\n",
            "weighted avg       0.89      0.77      0.78      6004\n",
            " samples avg       0.85      0.78      0.79      6004\n",
            "\n",
            "Hamming Loss:  0.103\n"
          ]
        }
      ],
      "source": [
        "# tfidf evaluation\n",
        "score_dict = {}\n",
        "for name, model in models_list:\n",
        "  if name == \"SVM\":\n",
        "    predict_proba = 0\n",
        "  else:\n",
        "    predict_proba = 1\n",
        "  print(f\"evaluating {name}\")\n",
        "  score_dict[name] = evaluate_chain(model, X_train_tfidf, y_train, X_test_tfidf, y_test, predict_proba = predict_proba)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Word2vec (Skipgram) evaluation"
      ],
      "metadata": {
        "id": "8Op49o__Znh_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGSg4Gn0MgrV",
        "outputId": "10724fd4-c0c2-4ad3-bced-60dbebb1f5ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating Logistic Regression\n",
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92      2721\n",
            "           1       0.90      0.73      0.81       581\n",
            "           2       0.81      0.57      0.67       518\n",
            "           3       0.86      0.79      0.82      1366\n",
            "           4       0.85      0.54      0.66       509\n",
            "           5       0.85      0.68      0.76       309\n",
            "\n",
            "   micro avg       0.87      0.81      0.84      6004\n",
            "   macro avg       0.86      0.71      0.77      6004\n",
            "weighted avg       0.87      0.81      0.83      6004\n",
            " samples avg       0.87      0.84      0.83      6004\n",
            "\n",
            "Hamming Loss:  0.091\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.93      2721\n",
            "           1       0.90      0.74      0.81       581\n",
            "           2       0.83      0.59      0.69       518\n",
            "           3       0.86      0.79      0.82      1366\n",
            "           4       0.85      0.54      0.66       509\n",
            "           5       0.87      0.64      0.74       309\n",
            "\n",
            "   micro avg       0.88      0.82      0.85      6004\n",
            "   macro avg       0.87      0.71      0.78      6004\n",
            "weighted avg       0.87      0.82      0.84      6004\n",
            " samples avg       0.88      0.84      0.84      6004\n",
            "\n",
            "Hamming Loss:  0.087\n",
            "evaluating XGBoost\n",
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.96      0.93      2721\n",
            "           1       0.89      0.77      0.83       581\n",
            "           2       0.88      0.70      0.78       518\n",
            "           3       0.86      0.84      0.85      1366\n",
            "           4       0.87      0.68      0.76       509\n",
            "           5       0.90      0.71      0.79       309\n",
            "\n",
            "   micro avg       0.88      0.86      0.87      6004\n",
            "   macro avg       0.88      0.78      0.82      6004\n",
            "weighted avg       0.88      0.86      0.87      6004\n",
            " samples avg       0.88      0.87      0.86      6004\n",
            "\n",
            "Hamming Loss:  0.075\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.98      0.93      2721\n",
            "           1       0.91      0.77      0.83       581\n",
            "           2       0.87      0.71      0.79       518\n",
            "           3       0.88      0.85      0.86      1366\n",
            "           4       0.89      0.68      0.77       509\n",
            "           5       0.89      0.70      0.79       309\n",
            "\n",
            "   micro avg       0.89      0.87      0.88      6004\n",
            "   macro avg       0.89      0.78      0.83      6004\n",
            "weighted avg       0.89      0.87      0.87      6004\n",
            " samples avg       0.89      0.88      0.87      6004\n",
            "\n",
            "Hamming Loss:  0.071\n",
            "evaluating SVM\n",
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.98      0.92      2721\n",
            "           1       0.92      0.66      0.77       581\n",
            "           2       0.89      0.42      0.57       518\n",
            "           3       0.84      0.80      0.82      1366\n",
            "           4       0.87      0.50      0.63       509\n",
            "           5       0.89      0.63      0.74       309\n",
            "\n",
            "   micro avg       0.86      0.80      0.83      6004\n",
            "   macro avg       0.88      0.67      0.74      6004\n",
            "weighted avg       0.87      0.80      0.82      6004\n",
            " samples avg       0.87      0.83      0.82      6004\n",
            "\n",
            "Hamming Loss:  0.096\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.99      0.92      2721\n",
            "           1       0.91      0.68      0.78       581\n",
            "           2       0.91      0.39      0.55       518\n",
            "           3       0.84      0.81      0.83      1366\n",
            "           4       0.87      0.51      0.64       509\n",
            "           5       0.89      0.62      0.73       309\n",
            "\n",
            "   micro avg       0.86      0.81      0.84      6004\n",
            "   macro avg       0.88      0.67      0.74      6004\n",
            "weighted avg       0.87      0.81      0.82      6004\n",
            " samples avg       0.87      0.84      0.83      6004\n",
            "\n",
            "Hamming Loss:  0.094\n"
          ]
        }
      ],
      "source": [
        "# word2vec (skipgram) evaluation\n",
        "models_list = [(\"Logistic Regression\", lr_model), (\"XGBoost\", xgb_model), (\"SVM\", sgd)]\n",
        "score_dict = {}\n",
        "for name, model in models_list:\n",
        "  if name == \"SVM\":\n",
        "    predict_proba = 0\n",
        "  else:\n",
        "    predict_proba = 1\n",
        "  print(f\"evaluating {name}\")\n",
        "  score_dict[name] = evaluate_chain(model, X_train_skipgram, y_train, X_test_skipgram, y_test, predict_proba = predict_proba)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Word2vec cbow evalutaion"
      ],
      "metadata": {
        "id": "3NdguAFWZt0I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT29UI0lRKev",
        "outputId": "f38df36e-2094-4d68-dc5c-d24a148d73ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating Logistic Regression\n",
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.95      0.92      2721\n",
            "           1       0.93      0.82      0.87       581\n",
            "           2       0.84      0.67      0.75       518\n",
            "           3       0.90      0.85      0.87      1366\n",
            "           4       0.90      0.65      0.75       509\n",
            "           5       0.86      0.73      0.79       309\n",
            "\n",
            "   micro avg       0.89      0.86      0.87      6004\n",
            "   macro avg       0.89      0.78      0.83      6004\n",
            "weighted avg       0.89      0.86      0.87      6004\n",
            " samples avg       0.89      0.87      0.86      6004\n",
            "\n",
            "Hamming Loss:  0.073\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.96      0.93      2721\n",
            "           1       0.93      0.83      0.88       581\n",
            "           2       0.86      0.68      0.76       518\n",
            "           3       0.90      0.85      0.87      1366\n",
            "           4       0.89      0.67      0.77       509\n",
            "           5       0.87      0.71      0.78       309\n",
            "\n",
            "   micro avg       0.90      0.86      0.88      6004\n",
            "   macro avg       0.89      0.78      0.83      6004\n",
            "weighted avg       0.89      0.86      0.87      6004\n",
            " samples avg       0.89      0.87      0.87      6004\n",
            "\n",
            "Hamming Loss:  0.071\n",
            "evaluating XGBoost\n",
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.96      0.93      2721\n",
            "           1       0.91      0.82      0.87       581\n",
            "           2       0.86      0.69      0.76       518\n",
            "           3       0.89      0.87      0.88      1366\n",
            "           4       0.90      0.69      0.78       509\n",
            "           5       0.90      0.70      0.78       309\n",
            "\n",
            "   micro avg       0.90      0.87      0.88      6004\n",
            "   macro avg       0.89      0.79      0.83      6004\n",
            "weighted avg       0.90      0.87      0.88      6004\n",
            " samples avg       0.89      0.88      0.87      6004\n",
            "\n",
            "Hamming Loss:  0.069\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.97      0.93      2721\n",
            "           1       0.93      0.84      0.88       581\n",
            "           2       0.86      0.69      0.77       518\n",
            "           3       0.89      0.87      0.88      1366\n",
            "           4       0.90      0.69      0.78       509\n",
            "           5       0.91      0.72      0.81       309\n",
            "\n",
            "   micro avg       0.90      0.88      0.89      6004\n",
            "   macro avg       0.90      0.80      0.84      6004\n",
            "weighted avg       0.90      0.88      0.88      6004\n",
            " samples avg       0.90      0.88      0.88      6004\n",
            "\n",
            "Hamming Loss:  0.066\n",
            "evaluating SVM\n",
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.92      0.92      2721\n",
            "           1       0.94      0.80      0.86       581\n",
            "           2       0.81      0.63      0.71       518\n",
            "           3       0.87      0.88      0.88      1366\n",
            "           4       0.86      0.67      0.75       509\n",
            "           5       0.84      0.73      0.78       309\n",
            "\n",
            "   micro avg       0.89      0.84      0.87      6004\n",
            "   macro avg       0.87      0.77      0.82      6004\n",
            "weighted avg       0.89      0.84      0.86      6004\n",
            " samples avg       0.88      0.86      0.85      6004\n",
            "\n",
            "Hamming Loss:  0.076\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.95      0.93      2721\n",
            "           1       0.93      0.81      0.87       581\n",
            "           2       0.83      0.61      0.70       518\n",
            "           3       0.87      0.88      0.88      1366\n",
            "           4       0.90      0.66      0.76       509\n",
            "           5       0.84      0.72      0.78       309\n",
            "\n",
            "   micro avg       0.89      0.86      0.87      6004\n",
            "   macro avg       0.88      0.77      0.82      6004\n",
            "weighted avg       0.89      0.86      0.87      6004\n",
            " samples avg       0.89      0.87      0.86      6004\n",
            "\n",
            "Hamming Loss:  0.074\n"
          ]
        }
      ],
      "source": [
        "# word2vec (cbow) evaluation\n",
        "score_dict = {}\n",
        "for name, model in models_list:\n",
        "  if name == \"SVM\":\n",
        "    predict_proba = 0\n",
        "  else:\n",
        "    predict_proba = 1\n",
        "  print(f\"evaluating {name}\")\n",
        "  score_dict[name] = evaluate_chain(model, X_train_cbow, y_train, X_test_cbow, y_test, predict_proba = predict_proba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEuhvCcCz1La"
      },
      "outputs": [],
      "source": [
        "phobert_test = pd.read_csv(\"/content/drive/MyDrive/Thesis: Topic Modelling/Code/Phobert result/phobert_test_feature.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
        "phobert_train = pd.read_csv(\"/content/drive/MyDrive/Thesis: Topic Modelling/Code/Phobert result/phobert_train_feature.csv\").drop(\"Unnamed: 0\", axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLplNgLkj8zD",
        "outputId": "9cf80755-67e8-4e82-a665-1495f1dd4e39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12240, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "phobert_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Evaluate embedding from Phobert"
      ],
      "metadata": {
        "id": "Y-sRjbonZzsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_best_path =  \"/content/drive/MyDrive/THESIS DSEB62: Product review analysis/Baseline-model/Phobertv2\" + \"/best.pth\"\n",
        "model = torch.load(save_best_path)\n",
        "embedder = model.roberta"
      ],
      "metadata": {
        "id": "MkOd-cNjE73L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p48N-683Kc-",
        "outputId": "722a06aa-5615-47f5-d9da-1d49ae432095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating Logistic Regression\n",
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.97      2721\n",
            "           1       0.98      0.98      0.98       581\n",
            "           2       0.94      0.93      0.94       518\n",
            "           3       0.94      0.97      0.96      1366\n",
            "           4       0.97      0.96      0.97       509\n",
            "           5       0.95      0.95      0.95       309\n",
            "\n",
            "   micro avg       0.96      0.97      0.96      6004\n",
            "   macro avg       0.96      0.96      0.96      6004\n",
            "weighted avg       0.96      0.97      0.96      6004\n",
            " samples avg       0.96      0.97      0.96      6004\n",
            "\n",
            "Hamming Loss:  0.022\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.97      2721\n",
            "           1       0.98      0.98      0.98       581\n",
            "           2       0.94      0.93      0.94       518\n",
            "           3       0.94      0.97      0.95      1366\n",
            "           4       0.97      0.96      0.97       509\n",
            "           5       0.95      0.95      0.95       309\n",
            "\n",
            "   micro avg       0.96      0.97      0.96      6004\n",
            "   macro avg       0.96      0.96      0.96      6004\n",
            "weighted avg       0.96      0.97      0.96      6004\n",
            " samples avg       0.96      0.97      0.96      6004\n",
            "\n",
            "Hamming Loss:  0.023\n",
            "evaluating XGBoost\n",
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.96      2721\n",
            "           1       0.98      0.98      0.98       581\n",
            "           2       0.94      0.92      0.93       518\n",
            "           3       0.94      0.96      0.95      1366\n",
            "           4       0.97      0.97      0.97       509\n",
            "           5       0.96      0.94      0.95       309\n",
            "\n",
            "   micro avg       0.96      0.96      0.96      6004\n",
            "   macro avg       0.96      0.96      0.96      6004\n",
            "weighted avg       0.96      0.96      0.96      6004\n",
            " samples avg       0.96      0.96      0.95      6004\n",
            "\n",
            "Hamming Loss:  0.024\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.96      2721\n",
            "           1       0.98      0.98      0.98       581\n",
            "           2       0.94      0.92      0.93       518\n",
            "           3       0.94      0.96      0.95      1366\n",
            "           4       0.97      0.97      0.97       509\n",
            "           5       0.96      0.94      0.95       309\n",
            "\n",
            "   micro avg       0.96      0.96      0.96      6004\n",
            "   macro avg       0.96      0.96      0.96      6004\n",
            "weighted avg       0.96      0.96      0.96      6004\n",
            " samples avg       0.96      0.96      0.96      6004\n",
            "\n",
            "Hamming Loss:  0.024\n",
            "evaluating SVM\n",
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97      2721\n",
            "           1       0.98      0.98      0.98       581\n",
            "           2       0.95      0.92      0.93       518\n",
            "           3       0.94      0.97      0.95      1366\n",
            "           4       0.96      0.97      0.97       509\n",
            "           5       0.93      0.96      0.94       309\n",
            "\n",
            "   micro avg       0.95      0.97      0.96      6004\n",
            "   macro avg       0.95      0.96      0.96      6004\n",
            "weighted avg       0.95      0.97      0.96      6004\n",
            " samples avg       0.96      0.97      0.96      6004\n",
            "\n",
            "Hamming Loss:  0.024\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.97      2721\n",
            "           1       0.98      0.98      0.98       581\n",
            "           2       0.95      0.91      0.93       518\n",
            "           3       0.94      0.98      0.96      1366\n",
            "           4       0.96      0.97      0.97       509\n",
            "           5       0.94      0.96      0.95       309\n",
            "\n",
            "   micro avg       0.95      0.97      0.96      6004\n",
            "   macro avg       0.95      0.96      0.96      6004\n",
            "weighted avg       0.95      0.97      0.96      6004\n",
            " samples avg       0.96      0.97      0.96      6004\n",
            "\n",
            "Hamming Loss:  0.023\n"
          ]
        }
      ],
      "source": [
        "# phobert embedding evaluation\n",
        "score_dict = {}\n",
        "for name, model in models_list:\n",
        "  if name == \"SVM\":\n",
        "    predict_proba = 0\n",
        "  else:\n",
        "    predict_proba = 1\n",
        "  print(f\"evaluating {name}\")\n",
        "  score_dict[name] = evaluate_chain(model, phobert_train, y_train, phobert_test, y_test, predict_proba = predict_proba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0QZSPAZ3yp4",
        "outputId": "6d0d9d7a-84be-4d9e-a556-349ef77ffdec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report from One Vs Rest model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97      2721\n",
            "           1       0.98      0.98      0.98       581\n",
            "           2       0.95      0.92      0.93       518\n",
            "           3       0.94      0.97      0.95      1366\n",
            "           4       0.96      0.97      0.97       509\n",
            "           5       0.93      0.96      0.94       309\n",
            "\n",
            "   micro avg       0.95      0.97      0.96      6004\n",
            "   macro avg       0.95      0.96      0.96      6004\n",
            "weighted avg       0.95      0.97      0.96      6004\n",
            " samples avg       0.96      0.97      0.96      6004\n",
            "\n",
            "Hamming Loss:  0.024\n",
            "Classification report from Ensemble model from classifier chain\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.97      2721\n",
            "           1       0.98      0.98      0.98       581\n",
            "           2       0.95      0.91      0.93       518\n",
            "           3       0.94      0.98      0.96      1366\n",
            "           4       0.96      0.97      0.97       509\n",
            "           5       0.94      0.96      0.95       309\n",
            "\n",
            "   micro avg       0.95      0.97      0.96      6004\n",
            "   macro avg       0.95      0.96      0.96      6004\n",
            "weighted avg       0.95      0.97      0.96      6004\n",
            " samples avg       0.96      0.97      0.96      6004\n",
            "\n",
            "Hamming Loss:  0.023\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.02362050377339998,\n",
              " 0.02342448299519749,\n",
              " 0.02298343624424189,\n",
              " 0.022885425855140643,\n",
              " 0.024355581691659314,\n",
              " 0.023277467411545624,\n",
              " 0.02401254532980496,\n",
              " 0.023669508967950604,\n",
              " 0.023277467411545624,\n",
              " 0.02381652455160247,\n",
              " 0.023473488189748114,\n",
              " 0.023130451827893757]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "evaluate_chain(sgd, phobert_train, y_train, phobert_test, y_test, predict_proba = predict_proba)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}